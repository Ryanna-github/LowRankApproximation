@ARTICLE{kernel,
  author={Jianbo Shi and Malik, J.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Normalized cuts and image segmentation}, 
  year={2000},
  volume={22},
  number={8},
  pages={888-905},
  doi={10.1109/34.868688}}
  
  
@inproceedings{meka,
  author = "Si Si AND Cho-Jui Hsieh AND Inderjit S. Dhillon",
  title = "Memory Efﬁcient Kernel Approximation",
  booktitle = "International Conference on Machine Learning (ICML)",
  page = "701–709",
  year = "2014",
  month = "jun",
  abstract = "The scalability of kernel machines is a big challenge when facing millions of samples due to storage and computation issues for large kernel matrices, that are usually dense. Recently, many papers have suggested tackling this problem by using a low-rank approximation of the kernel matrix. In this paper, we first make the observation that the structure of shift-invariant kernels changes from low-rank to block-diagonal (without any low-rank structure) when varying the scale parameter. Based on this observation, we propose a new kernel approximation algorithm — Memory Efficient Kernel Approximation, which considers both low-rank and clustering structure of the kernel matrix. We show that the resulting algorithm outperforms state-of-the-art low-rank kernel approximation methods in terms of speed, approximation error, and memory usage. As an example, on the MNIST dataset with two-million samples, our method takes 550 seconds on a single machine using less than 500 MBytes memory to achieve 0.2313 test RMSE for kernel ridge regression, while standard Nystr{"}{o}m approximation takes more than 2700 seconds and uses more than 2 GBytes memory on the same problem to achieve 0.2318 test RMSE."
}

@book{algebra,
  author = "Hogben, L. (Ed.)",
  title =  "Handbook of Linear Algebra (2nd ed.)",
  year = "2013",
  doi = "https://doi.org/10.1201/b16113",
  publisher = "Chapman and Hall/CRC."
 }
 
 @misc{svd_rank,
  author = {Hopcroft and Kannan},
  title = {Lecture Notes (Forthcoming book)},
  howpublished = {[EB/OL]},
  note = {\url{https://www.cs.princeton.edu/courses/archive/spring12/cos598C/svdchapter.pdf}},
  year = "2012"
}

@inproceedings{nm,
  title={Using the Nystr{\"o}m Method to Speed Up Kernel Machines},
  author={Christopher K. I. Williams and Matthias W. Seeger},
  booktitle={NIPS},
  year={2000}
}

@inproceedings{nmsemble,
title = "Ensemble nystrom method",
author = "Sanjiv Kumar and Mehryar Mohri and Ameet Talwalkar",
year = "2009",
language = "English (US)",
isbn = "9781615679119",
series = "Advances in Neural Information Processing Systems 22 - Proceedings of the 2009 Conference",
pages = "1060--1068",
booktitle = "Advances in Neural Information Processing Systems 22 - Proceedings of the 2009 Conference",
note = "23rd Annual Conference on Neural Information Processing Systems, NIPS 2009 ; Conference date: 07-12-2009 Through 10-12-2009",
}

@ARTICLE{net,
    author = {Felipe Cucker and Steve Smale},
    title = {On the mathematical foundations of learning},
    journal = {Bulletin of the American Mathematical Society},
    year = {2002},
    volume = {39},
    pages = {1--49}
}





